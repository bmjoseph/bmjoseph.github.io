<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.53" />


<title>Log Loss as a Measure of Accuracy - Blog_Name</title>
<meta property="og:title" content="Log Loss as a Measure of Accuracy - Blog_Name">



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/bmjoseph">GitHub</a></li>
    
    <li><a href="https://www.linkedin.com/in/baileymjoseph/">LinkedIn</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">13 min read</span>
    

    <h1 class="article-title">Log Loss as a Measure of Accuracy</h1>

    
    <span class="article-date">2019/01/09</span>
    

    <div class="article-content">
      
<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/plotly-binding/plotly.js"></script>
<script src="/rmarkdown-libs/typedarray/typedarray.min.js"></script>
<script src="/rmarkdown-libs/jquery/jquery.min.js"></script>
<link href="/rmarkdown-libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/crosstalk/js/crosstalk.min.js"></script>
<link href="/rmarkdown-libs/plotlyjs/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="/rmarkdown-libs/plotlyjs/plotly-latest.min.js"></script>


<div id="motivation" class="section level2">
<h2>Motivation</h2>
<p>Evaluating a binary classifier can be difficult. Suppose you have a model which takes in some data and predicts whether a certain team will beat their opponent. Let’s say that specifically, it returns the probability that the home team wins. How do you know if your model is doing a good job?</p>
<p>What about just a standard accuracy? At first glance, it’s nice to say something like: “My model predicts the correct outcome 80% of the time.” You could easily get this by rounding the predictions and guessing win if <span class="math inline">\(p \ge .5\)</span> and loss if <span class="math inline">\(p \lt .5\)</span>.</p>
<p>The problem with this is that it you lose a lot of the information in the output. There should be a difference between being wrong when you guess <span class="math inline">\(p = .55\)</span> and being wrong when you guess <span class="math inline">\(p = .98\)</span>.</p>
<p>I think you should still report the accuracy – it’s easy for people to understand and works as a rough summary. But I’d like to introduce another possile scoring method – log loss.</p>
</div>
<div id="background" class="section level2">
<h2>Background</h2>
<p>I won’t go into the details of how log loss is defined mathematically. You can read about it <a href="https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression">here</a> if you’d like.</p>
<p>For now, let’s just assume we have an implementation:</p>
<pre class="r"><code>log_loss_binary &lt;- function(acutal, predicted, eps = 1e-15) {
  # Descritpion:   Evaluate a classifier by returning log loss
  # Inputs: 
  #   - actual:    vector of 1s and 0s. These should be the true results
  #   - predicted: vector of predicted probabilities of success (1)
  #   - eps:       numeric value, ensures loss is never infinite
  # Returns: 
  #   - log_loss:  numeric value, represents classifier error
  ...
}</code></pre>
</div>
<div id="log-loss-prefers-conservative-predictions" class="section level2">
<h2>Log Loss Prefers Conservative Predictions</h2>
<p>Log loss works by penalizing <em>all</em> of the predictions (unless you predict 100% or 0% and get it right). As long as you don’t predict 100% or 0%, the prediction will always be off from the real, binary, label. It makes snese as an evaluator because the penalty is larger the farther off your prediction turns out to be.</p>
<p>What does this mean on a sample of 1? We can visualize it below:</p>
<div id="759b340d52e4" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="759b340d52e4">{"x":{"data":[{"x":[0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99],"y":[0.0100503358535015,0.0202027073175195,0.0304592074847086,0.0408219945202552,0.0512932943875506,0.0618754037180875,0.0725706928348354,0.083381608939051,0.0943106794712413,0.105360515657826,0.116533816255952,0.127833371509885,0.139262067333508,0.150822889734584,0.162518929497775,0.174353387144778,0.186329578191493,0.198450938723838,0.210721031315653,0.22314355131421,0.23572233352107,0.2484613592985,0.261364764134408,0.27443684570176,0.287682072451781,0.301105092783922,0.3147107448397,0.328504066972036,0.342490308946776,0.356674943938732,0.371063681390832,0.385662480811985,0.400477566597125,0.415515443961666,0.430782916092454,0.44628710262842,0.462035459596559,0.478035800943,0.49429632181478,0.510825623765991,0.527632742082372,0.544727175441672,0.562118918153541,0.579818495252942,0.59783700075562,0.616186139423817,0.63487827243597,0.653926467406664,0.673344553263766,0.693147180559945,0.713349887877465,0.7339691750802,0.755022584278033,0.776528789498996,0.798507696217772,0.82098055206983,0.843970070294529,0.867500567704723,0.891598119283784,0.916290731874155,0.941608539858445,0.967584026261706,0.994252273343867,1.02165124753198,1.04982212449868,1.07880966137193,1.10866262452161,1.13943428318837,1.17118298150295,1.20397280432594,1.23787435600162,1.27296567581289,1.30933331998376,1.34707364796661,1.38629436111989,1.42711635564015,1.46967597005894,1.51412773262978,1.56064774826467,1.6094379124341,1.66073120682165,1.71479842809193,1.77195684193188,1.83258146374831,1.89711998488588,1.96611285637283,2.04022082852655,2.12026353620009,2.20727491318972,2.30258509299405,2.40794560865187,2.52572864430826,2.65926003693278,2.81341071676004,2.99573227355399,3.2188758248682,3.50655789731998,3.91202300542815,4.60517018598809],"text":["Predicted: 0.01<br />Log Loss: 0.01","Predicted: 0.02<br />Log Loss: 0.02","Predicted: 0.03<br />Log Loss: 0.03","Predicted: 0.04<br />Log Loss: 0.041","Predicted: 0.05<br />Log Loss: 0.051","Predicted: 0.06<br />Log Loss: 0.062","Predicted: 0.07<br />Log Loss: 0.073","Predicted: 0.08<br />Log Loss: 0.083","Predicted: 0.09<br />Log Loss: 0.094","Predicted: 0.1<br />Log Loss: 0.105","Predicted: 0.11<br />Log Loss: 0.117","Predicted: 0.12<br />Log Loss: 0.128","Predicted: 0.13<br />Log Loss: 0.139","Predicted: 0.14<br />Log Loss: 0.151","Predicted: 0.15<br />Log Loss: 0.163","Predicted: 0.16<br />Log Loss: 0.174","Predicted: 0.17<br />Log Loss: 0.186","Predicted: 0.18<br />Log Loss: 0.198","Predicted: 0.19<br />Log Loss: 0.211","Predicted: 0.2<br />Log Loss: 0.223","Predicted: 0.21<br />Log Loss: 0.236","Predicted: 0.22<br />Log Loss: 0.248","Predicted: 0.23<br />Log Loss: 0.261","Predicted: 0.24<br />Log Loss: 0.274","Predicted: 0.25<br />Log Loss: 0.288","Predicted: 0.26<br />Log Loss: 0.301","Predicted: 0.27<br />Log Loss: 0.315","Predicted: 0.28<br />Log Loss: 0.329","Predicted: 0.29<br />Log Loss: 0.342","Predicted: 0.3<br />Log Loss: 0.357","Predicted: 0.31<br />Log Loss: 0.371","Predicted: 0.32<br />Log Loss: 0.386","Predicted: 0.33<br />Log Loss: 0.4","Predicted: 0.34<br />Log Loss: 0.416","Predicted: 0.35<br />Log Loss: 0.431","Predicted: 0.36<br />Log Loss: 0.446","Predicted: 0.37<br />Log Loss: 0.462","Predicted: 0.38<br />Log Loss: 0.478","Predicted: 0.39<br />Log Loss: 0.494","Predicted: 0.4<br />Log Loss: 0.511","Predicted: 0.41<br />Log Loss: 0.528","Predicted: 0.42<br />Log Loss: 0.545","Predicted: 0.43<br />Log Loss: 0.562","Predicted: 0.44<br />Log Loss: 0.58","Predicted: 0.45<br />Log Loss: 0.598","Predicted: 0.46<br />Log Loss: 0.616","Predicted: 0.47<br />Log Loss: 0.635","Predicted: 0.48<br />Log Loss: 0.654","Predicted: 0.49<br />Log Loss: 0.673","Predicted: 0.5<br />Log Loss: 0.693","Predicted: 0.51<br />Log Loss: 0.713","Predicted: 0.52<br />Log Loss: 0.734","Predicted: 0.53<br />Log Loss: 0.755","Predicted: 0.54<br />Log Loss: 0.777","Predicted: 0.55<br />Log Loss: 0.799","Predicted: 0.56<br />Log Loss: 0.821","Predicted: 0.57<br />Log Loss: 0.844","Predicted: 0.58<br />Log Loss: 0.868","Predicted: 0.59<br />Log Loss: 0.892","Predicted: 0.6<br />Log Loss: 0.916","Predicted: 0.61<br />Log Loss: 0.942","Predicted: 0.62<br />Log Loss: 0.968","Predicted: 0.63<br />Log Loss: 0.994","Predicted: 0.64<br />Log Loss: 1.022","Predicted: 0.65<br />Log Loss: 1.05","Predicted: 0.66<br />Log Loss: 1.079","Predicted: 0.67<br />Log Loss: 1.109","Predicted: 0.68<br />Log Loss: 1.139","Predicted: 0.69<br />Log Loss: 1.171","Predicted: 0.7<br />Log Loss: 1.204","Predicted: 0.71<br />Log Loss: 1.238","Predicted: 0.72<br />Log Loss: 1.273","Predicted: 0.73<br />Log Loss: 1.309","Predicted: 0.74<br />Log Loss: 1.347","Predicted: 0.75<br />Log Loss: 1.386","Predicted: 0.76<br />Log Loss: 1.427","Predicted: 0.77<br />Log Loss: 1.47","Predicted: 0.78<br />Log Loss: 1.514","Predicted: 0.79<br />Log Loss: 1.561","Predicted: 0.8<br />Log Loss: 1.609","Predicted: 0.81<br />Log Loss: 1.661","Predicted: 0.82<br />Log Loss: 1.715","Predicted: 0.83<br />Log Loss: 1.772","Predicted: 0.84<br />Log Loss: 1.833","Predicted: 0.85<br />Log Loss: 1.897","Predicted: 0.86<br />Log Loss: 1.966","Predicted: 0.87<br />Log Loss: 2.04","Predicted: 0.88<br />Log Loss: 2.12","Predicted: 0.89<br />Log Loss: 2.207","Predicted: 0.9<br />Log Loss: 2.303","Predicted: 0.91<br />Log Loss: 2.408","Predicted: 0.92<br />Log Loss: 2.526","Predicted: 0.93<br />Log Loss: 2.659","Predicted: 0.94<br />Log Loss: 2.813","Predicted: 0.95<br />Log Loss: 2.996","Predicted: 0.96<br />Log Loss: 3.219","Predicted: 0.97<br />Log Loss: 3.507","Predicted: 0.98<br />Log Loss: 3.912","Predicted: 0.99<br />Log Loss: 4.605"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(248,118,109,1)","opacity":1,"size":5.66929133858268,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(248,118,109,1)"}},"hoveron":"points","name":"Lose","legendgroup":"Lose","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99],"y":[4.60517018598809,3.91202300542815,3.50655789731998,3.2188758248682,2.99573227355399,2.81341071676004,2.65926003693278,2.52572864430826,2.40794560865187,2.30258509299405,2.20727491318972,2.12026353620009,2.04022082852655,1.96611285637283,1.89711998488588,1.83258146374831,1.77195684193188,1.71479842809193,1.66073120682165,1.6094379124341,1.56064774826467,1.51412773262978,1.46967597005894,1.42711635564015,1.38629436111989,1.34707364796661,1.30933331998376,1.27296567581289,1.23787435600162,1.20397280432594,1.17118298150295,1.13943428318836,1.10866262452161,1.07880966137193,1.04982212449868,1.02165124753198,0.994252273343867,0.967584026261706,0.941608539858445,0.916290731874155,0.891598119283784,0.867500567704723,0.843970070294529,0.82098055206983,0.798507696217772,0.776528789498996,0.755022584278033,0.7339691750802,0.713349887877465,0.693147180559945,0.673344553263766,0.653926467406664,0.63487827243597,0.616186139423817,0.59783700075562,0.579818495252942,0.562118918153541,0.544727175441672,0.527632742082372,0.510825623765991,0.49429632181478,0.478035800943,0.462035459596559,0.446287102628419,0.430782916092454,0.415515443961666,0.400477566597125,0.385662480811985,0.371063681390832,0.356674943938732,0.342490308946776,0.328504066972036,0.3147107448397,0.301105092783922,0.287682072451781,0.27443684570176,0.261364764134408,0.2484613592985,0.23572233352107,0.22314355131421,0.210721031315653,0.198450938723838,0.186329578191493,0.174353387144778,0.162518929497775,0.150822889734584,0.139262067333508,0.127833371509885,0.116533816255952,0.105360515657826,0.0943106794712413,0.083381608939051,0.0725706928348354,0.0618754037180874,0.0512932943875505,0.040821994520255,0.0304592074847086,0.0202027073175195,0.0100503358535015],"text":["Predicted: 0.01<br />Log Loss: 4.605","Predicted: 0.02<br />Log Loss: 3.912","Predicted: 0.03<br />Log Loss: 3.507","Predicted: 0.04<br />Log Loss: 3.219","Predicted: 0.05<br />Log Loss: 2.996","Predicted: 0.06<br />Log Loss: 2.813","Predicted: 0.07<br />Log Loss: 2.659","Predicted: 0.08<br />Log Loss: 2.526","Predicted: 0.09<br />Log Loss: 2.408","Predicted: 0.1<br />Log Loss: 2.303","Predicted: 0.11<br />Log Loss: 2.207","Predicted: 0.12<br />Log Loss: 2.12","Predicted: 0.13<br />Log Loss: 2.04","Predicted: 0.14<br />Log Loss: 1.966","Predicted: 0.15<br />Log Loss: 1.897","Predicted: 0.16<br />Log Loss: 1.833","Predicted: 0.17<br />Log Loss: 1.772","Predicted: 0.18<br />Log Loss: 1.715","Predicted: 0.19<br />Log Loss: 1.661","Predicted: 0.2<br />Log Loss: 1.609","Predicted: 0.21<br />Log Loss: 1.561","Predicted: 0.22<br />Log Loss: 1.514","Predicted: 0.23<br />Log Loss: 1.47","Predicted: 0.24<br />Log Loss: 1.427","Predicted: 0.25<br />Log Loss: 1.386","Predicted: 0.26<br />Log Loss: 1.347","Predicted: 0.27<br />Log Loss: 1.309","Predicted: 0.28<br />Log Loss: 1.273","Predicted: 0.29<br />Log Loss: 1.238","Predicted: 0.3<br />Log Loss: 1.204","Predicted: 0.31<br />Log Loss: 1.171","Predicted: 0.32<br />Log Loss: 1.139","Predicted: 0.33<br />Log Loss: 1.109","Predicted: 0.34<br />Log Loss: 1.079","Predicted: 0.35<br />Log Loss: 1.05","Predicted: 0.36<br />Log Loss: 1.022","Predicted: 0.37<br />Log Loss: 0.994","Predicted: 0.38<br />Log Loss: 0.968","Predicted: 0.39<br />Log Loss: 0.942","Predicted: 0.4<br />Log Loss: 0.916","Predicted: 0.41<br />Log Loss: 0.892","Predicted: 0.42<br />Log Loss: 0.868","Predicted: 0.43<br />Log Loss: 0.844","Predicted: 0.44<br />Log Loss: 0.821","Predicted: 0.45<br />Log Loss: 0.799","Predicted: 0.46<br />Log Loss: 0.777","Predicted: 0.47<br />Log Loss: 0.755","Predicted: 0.48<br />Log Loss: 0.734","Predicted: 0.49<br />Log Loss: 0.713","Predicted: 0.5<br />Log Loss: 0.693","Predicted: 0.51<br />Log Loss: 0.673","Predicted: 0.52<br />Log Loss: 0.654","Predicted: 0.53<br />Log Loss: 0.635","Predicted: 0.54<br />Log Loss: 0.616","Predicted: 0.55<br />Log Loss: 0.598","Predicted: 0.56<br />Log Loss: 0.58","Predicted: 0.57<br />Log Loss: 0.562","Predicted: 0.58<br />Log Loss: 0.545","Predicted: 0.59<br />Log Loss: 0.528","Predicted: 0.6<br />Log Loss: 0.511","Predicted: 0.61<br />Log Loss: 0.494","Predicted: 0.62<br />Log Loss: 0.478","Predicted: 0.63<br />Log Loss: 0.462","Predicted: 0.64<br />Log Loss: 0.446","Predicted: 0.65<br />Log Loss: 0.431","Predicted: 0.66<br />Log Loss: 0.416","Predicted: 0.67<br />Log Loss: 0.4","Predicted: 0.68<br />Log Loss: 0.386","Predicted: 0.69<br />Log Loss: 0.371","Predicted: 0.7<br />Log Loss: 0.357","Predicted: 0.71<br />Log Loss: 0.342","Predicted: 0.72<br />Log Loss: 0.329","Predicted: 0.73<br />Log Loss: 0.315","Predicted: 0.74<br />Log Loss: 0.301","Predicted: 0.75<br />Log Loss: 0.288","Predicted: 0.76<br />Log Loss: 0.274","Predicted: 0.77<br />Log Loss: 0.261","Predicted: 0.78<br />Log Loss: 0.248","Predicted: 0.79<br />Log Loss: 0.236","Predicted: 0.8<br />Log Loss: 0.223","Predicted: 0.81<br />Log Loss: 0.211","Predicted: 0.82<br />Log Loss: 0.198","Predicted: 0.83<br />Log Loss: 0.186","Predicted: 0.84<br />Log Loss: 0.174","Predicted: 0.85<br />Log Loss: 0.163","Predicted: 0.86<br />Log Loss: 0.151","Predicted: 0.87<br />Log Loss: 0.139","Predicted: 0.88<br />Log Loss: 0.128","Predicted: 0.89<br />Log Loss: 0.117","Predicted: 0.9<br />Log Loss: 0.105","Predicted: 0.91<br />Log Loss: 0.094","Predicted: 0.92<br />Log Loss: 0.083","Predicted: 0.93<br />Log Loss: 0.073","Predicted: 0.94<br />Log Loss: 0.062","Predicted: 0.95<br />Log Loss: 0.051","Predicted: 0.96<br />Log Loss: 0.041","Predicted: 0.97<br />Log Loss: 0.03","Predicted: 0.98<br />Log Loss: 0.02","Predicted: 0.99<br />Log Loss: 0.01"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(0,191,196,1)","opacity":1,"size":5.66929133858268,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(0,191,196,1)"}},"hoveron":"points","name":"Win","legendgroup":"Win","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":43.7625570776256,"r":7.30593607305936,"b":40.1826484018265,"l":31.4155251141553},"plot_bgcolor":"rgba(255,255,255,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"title":"Log Loss for a Single Prediction","titlefont":{"color":"rgba(0,0,0,1)","family":"","size":17.5342465753425},"xaxis":{"domain":[0,1],"type":"linear","autorange":false,"tickmode":"array","range":[-0.039,1.039],"ticktext":["0.00","0.25","0.50","0.75","1.00"],"tickvals":[0,0.25,0.5,0.75,1],"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":false,"gridcolor":null,"gridwidth":0,"zeroline":false,"anchor":"y","title":"Predicted Probability of Success","titlefont":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"type":"linear","autorange":false,"tickmode":"array","range":[-0.219705656653228,4.83492617849482],"ticktext":["0","1","2","3","4"],"tickvals":[0,1,2,3,4],"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":"Log Loss","titlefont":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":true,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895},"y":0.913385826771654},"annotations":[{"text":"Result","x":1.02,"y":1,"showarrow":false,"ax":0,"ay":0,"font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xref":"paper","yref":"paper","textangle":-0,"xanchor":"left","yanchor":"bottom","legendTitle":true}],"hovermode":"closest"},"source":"A","attrs":{"759b6c839a64":{"text":{},"x":{},"y":{},"colour":{},"type":"ggplotly"}},"cur_data":"759b6c839a64","visdat":{"759b6c839a64":["function (y) ","x"]},"config":{"modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>
<p>As we should hope, if the result is a win, then the log loss is low for predictions near 1. The reverse is also true.</p>
<p>We’ve also found the first subtlety of log loss – there are significant diminishing returns to your classfier being more confident. If your team is <em>going</em> to win the game, of course it’s better if the classifier predicts .95 (log loss <span class="math inline">\(\approx .05\)</span>) rather than .75 (log loss <span class="math inline">\(\approx .29\)</span>), but predicting .95 and being wrong is disastrous (log loss <span class="math inline">\(\approx 3\)</span>).</p>
<p>How much does this affect things? Presumably, if the team has a 95% chance to win every game, then over the long run predicting .95 for every game should be better than predicting 75%, even though there’ll be a high penalty on the games we get unlucky. Let’s check this:</p>
<pre><code>## [1] &quot;Log Loss for predicting 95%: 0.199&quot;</code></pre>
<pre><code>## [1] &quot;Log Loss for predicting 75%: 0.343&quot;</code></pre>
<p>Okay, that’s good. But what if the real win probability is somewhere between .75 and .95? How high does the win probability have to be for guessing 95% for every game to be better than guessing 75%?</p>
<div id="759b7e6c59cf" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="759b7e6c59cf">{"x":{"data":[{"x":[0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95],"y":[0.562335144618808,0.551349021732127,0.540362898845446,0.529376775958765,0.518390653072084,0.507404530185403,0.496358181888044,0.485432284412041,0.47444616152536,0.463460038638678,0.452473915751997,0.441487792865316,0.430501669978635,0.419515547091954,0.408529424205273,0.397543301318592,0.386557178431911,0.37557105554523,0.364584932658549,0.353598809771867,0.342612686885186],"text":["Win Prob: 0.75<br>Log Loss: 0.562","Win Prob: 0.76<br>Log Loss: 0.551","Win Prob: 0.77<br>Log Loss: 0.54","Win Prob: 0.78<br>Log Loss: 0.529","Win Prob: 0.79<br>Log Loss: 0.518","Win Prob: 0.8<br>Log Loss: 0.507","Win Prob: 0.81<br>Log Loss: 0.496","Win Prob: 0.82<br>Log Loss: 0.485","Win Prob: 0.83<br>Log Loss: 0.474","Win Prob: 0.84<br>Log Loss: 0.463","Win Prob: 0.85<br>Log Loss: 0.452","Win Prob: 0.86<br>Log Loss: 0.441","Win Prob: 0.87<br>Log Loss: 0.431","Win Prob: 0.88<br>Log Loss: 0.42","Win Prob: 0.89<br>Log Loss: 0.409","Win Prob: 0.9<br>Log Loss: 0.398","Win Prob: 0.91<br>Log Loss: 0.387","Win Prob: 0.92<br>Log Loss: 0.376","Win Prob: 0.93<br>Log Loss: 0.365","Win Prob: 0.94<br>Log Loss: 0.354","Win Prob: 0.95<br>Log Loss: 0.343"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(248,118,109,1)","opacity":1,"size":5.66929133858268,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(248,118,109,1)"}},"hoveron":"points","name":"75%","legendgroup":"75%","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95],"y":[0.78740303917916,0.757958649387496,0.728514259595832,0.699069869804167,0.669625480012503,0.640181090220838,0.610503306861944,0.58129231063751,0.551847920845845,0.522403531054181,0.492959141262516,0.463514751470852,0.434070361679188,0.404625971887523,0.375181582095859,0.345737192304195,0.31629280251253,0.286848412720866,0.257404022929201,0.227959633137537,0.198515243345873],"text":["Win Prob: 0.75<br>Log Loss: 0.787","Win Prob: 0.76<br>Log Loss: 0.758","Win Prob: 0.77<br>Log Loss: 0.729","Win Prob: 0.78<br>Log Loss: 0.699","Win Prob: 0.79<br>Log Loss: 0.67","Win Prob: 0.8<br>Log Loss: 0.64","Win Prob: 0.81<br>Log Loss: 0.611","Win Prob: 0.82<br>Log Loss: 0.581","Win Prob: 0.83<br>Log Loss: 0.552","Win Prob: 0.84<br>Log Loss: 0.522","Win Prob: 0.85<br>Log Loss: 0.493","Win Prob: 0.86<br>Log Loss: 0.464","Win Prob: 0.87<br>Log Loss: 0.434","Win Prob: 0.88<br>Log Loss: 0.405","Win Prob: 0.89<br>Log Loss: 0.375","Win Prob: 0.9<br>Log Loss: 0.346","Win Prob: 0.91<br>Log Loss: 0.316","Win Prob: 0.92<br>Log Loss: 0.287","Win Prob: 0.93<br>Log Loss: 0.257","Win Prob: 0.94<br>Log Loss: 0.228","Win Prob: 0.95<br>Log Loss: 0.199"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(0,191,196,1)","opacity":1,"size":5.66929133858268,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(0,191,196,1)"}},"hoveron":"points","name":"95%","legendgroup":"95%","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":43.7625570776256,"r":7.30593607305936,"b":40.1826484018265,"l":43.1050228310502},"plot_bgcolor":"rgba(255,255,255,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"title":"Log Loss Favors Conservative Predictions","titlefont":{"color":"rgba(0,0,0,1)","family":"","size":17.5342465753425},"xaxis":{"domain":[0,1],"type":"linear","autorange":false,"tickmode":"array","range":[0.74,0.96],"ticktext":["0.75","0.80","0.85","0.90","0.95"],"tickvals":[0.75,0.8,0.85,0.9,0.95],"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":false,"gridcolor":null,"gridwidth":0,"zeroline":false,"anchor":"y","title":"Real Win Probability","titlefont":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"type":"linear","autorange":false,"tickmode":"array","range":[0.169070853554208,0.816847428970825],"ticktext":["0.2","0.4","0.6","0.8"],"tickvals":[0.2,0.4,0.6,0.8],"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":"Log Loss","titlefont":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":true,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895},"y":0.913385826771654},"annotations":[{"text":"Prediction","x":1.02,"y":1,"showarrow":false,"ax":0,"ay":0,"font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xref":"paper","yref":"paper","textangle":-0,"xanchor":"left","yanchor":"bottom","legendTitle":true}],"hovermode":"closest"},"source":"A","attrs":{"759b481f4ade":{"text":{},"x":{},"y":{},"colour":{},"type":"ggplotly"}},"cur_data":"759b481f4ade","visdat":{"759b481f4ade":["function (y) ","x"]},"config":{"modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>
<p>The 95% predictions only start to be better than the 75% predictions when the real win probability is 88%.</p>
</div>
<div id="log-loss-doesnt-care-about-bias" class="section level2">
<h2>Log Loss Doesn’t Care About Bias</h2>
<p>There’s one more thing that you should know before you decide to use log loss on your own projects. Log loss doesn’t care whether your predictions are biased or unbiased.</p>
<p>If you predict 50% on every entry, the actual distribution of results doesn’t affect your log loss at all:</p>
<pre class="r"><code>log_loss_binary(actual = c(1, 0), predicted = c(.5, .5))</code></pre>
<pre><code>## [1] 0.6931472</code></pre>
<pre class="r"><code>log_loss_binary(actual = c(1, 1), predicted = c(.5, .5))</code></pre>
<pre><code>## [1] 0.6931472</code></pre>
<pre class="r"><code>log_loss_binary(actual = c(0, 0), predicted = c(.5, .5))</code></pre>
<pre><code>## [1] 0.6931472</code></pre>
<p>All of these numbers are the same! This is because log loss looks at one game at a time. Regardless of the outcome of the game, your prediction is off by 50% because <span class="math inline">\(\left | .5 - 0 \right | = \left | .5 - 1 \right | = .5\)</span>.</p>
<p>This property takes some getting used to, but it can be either beneficial or harmful. Suppose your classifier geusses 51% on every game:</p>
<table>
<thead>
<tr class="header">
<th align="center">Real Win Probability</th>
<th align="center">Accuracy</th>
<th align="center">Log Loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0</td>
<td align="center">0</td>
<td align="center">.7133</td>
</tr>
<tr class="even">
<td align="center">.5</td>
<td align="center">.5</td>
<td align="center">.6933</td>
</tr>
<tr class="odd">
<td align="center">1</td>
<td align="center">1</td>
<td align="center">.6733</td>
</tr>
</tbody>
</table>
<p>The plus side is that the log loss is more stable than accuracy with respect to the real win probability. The classifier has made only very weak predictions, so it shouldn’t be praised with 100% accuracy if it happens to be correct.</p>
<p>On the other hand, notice that the log loss is actually <em>lower</em> if the real win probability is 100% rather than 50%. If the real win probability is 100%, then the classifier has an error of 49% every time. If the real win probability is 50%, then the classifier has an error of either 49% or 51%.</p>
<p>For this reason, I highly recommend looking at your model’s bias alongside its log loss. One simple way of checking the bias is to add up all the predictions and compare that number to the sum of the actual results. If we assume there were 10 games in the above table, we have:</p>
<table>
<thead>
<tr class="header">
<th align="center">Real Win Probability</th>
<th align="center">Accuracy</th>
<th align="center">Log Loss</th>
<th align="center">Actual Wins</th>
<th align="center">Predicted Wins</th>
<th align="center">Bias</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0</td>
<td align="center">0</td>
<td align="center">.7133</td>
<td align="center">0</td>
<td align="center">5.1</td>
<td align="center">-5.1</td>
</tr>
<tr class="even">
<td align="center">.5</td>
<td align="center">.5</td>
<td align="center">.6933</td>
<td align="center">5</td>
<td align="center">5.1</td>
<td align="center">-0.1</td>
</tr>
<tr class="odd">
<td align="center">1</td>
<td align="center">1</td>
<td align="center">.6733</td>
<td align="center">10</td>
<td align="center">5.1</td>
<td align="center">+4.9</td>
</tr>
</tbody>
</table>
<p>The combination of accuracy, loss and bias can give us a good idea about how well our classifier is working.</p>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

